{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify normalized skills in CVs\n",
    "\n",
    "Extracting normalized skills from CVs **simplifies the hiring process** by allowing recruiters to quickly identify candidates with the desired skills. \n",
    "While this is currently an open problem, this can be tackled in specific sectors using language processors and interfaces to support applicants in writing their CVs\n",
    "and applying for jobs.\n",
    "\n",
    "Normalization allows providing a **consistent context** to be passed to AI models and to humans for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Large platforms like linkedin have a large amount of data and significant experience in this area.\n",
    "On the other hand, a more focused approach for specific tasks can be more effective, especially when evaluating a large number of CVs (e.g. for junior positions).\n",
    "Moreover, a standardized approach can be more effective in the long term, since it allows to build a common knowledge base to support the relocation of workers in time\n",
    "(e.g. identifying skill gaps and training needs).\n",
    "\n",
    "ESCO is a multilingual classification of skills, competences, qualifications and occupations maintained and updated periodically by the European Commission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the ESCO dataset\n",
    "\n",
    "```mermaid\n",
    "---\n",
    "title: ESCO dataset\n",
    "---\n",
    "graph\n",
    "\n",
    "SK[Skill]\n",
    "O[Occupation]\n",
    "K[Knowledge]\n",
    "\n",
    "SK -->|broader| SK\n",
    "\n",
    "O -->|essentialSkill 1..n| SK\n",
    "O -->|optionalSkill 1..n| SK\n",
    "\n",
    "SK -.->|type| S\n",
    "SK -.->|type| K\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Structure of Skills and Occupations\n",
    "\n",
    "Here is an excerpt from the **SaaS (service-oriented modelling)** skill:\n",
    "\n",
    "- preferred label: SaaS (service-oriented modelling)\n",
    "- unique identifier as an URI: http://data.europa.eu/esco/skill/eeca3780-8049-499f-a268-95a7ad26642c\n",
    "- alternative labels: SaaS model\n",
    "- description: The SaaS model consists of principles and fundamentals of service-oriented modelling for business and software systems that allow the design and specification of service-oriented business systems within a variety of architectural styles, such as enterprise architecture.\n",
    "\n",
    "\n",
    "While occupations are something that changes in time, skills are more stable.\n",
    "Nonetheless, the ESCO Occupation taxonomy can be useful as a reference for writing position and the associated skills.\n",
    "\n",
    "An ESCO Occupation excerpt for **Cloud engineer**. Note that the URIs of the skills are present in the dataset but not shown below:\n",
    "\n",
    "- preferred label: cloud engineer\n",
    "- unique identifier as an URI: http://data.europa.eu/esco/occupation/349ee6f6-c295-4c38-9b98-48765b55280e\n",
    "- alternative labels: cloud-native engineer, cloud architect, cloud computing engineer, cloud developer, cloud devops engineer, cloud infrastructure engineer, cloud network engineer, cloud security engineer, cloud software engineer, cloud solution engineer, hybrid cloud engineer\n",
    "- description: Cloud engineers are responsible for the design, planning, management and maintenance of cloud-based systems. They develop and implement cloud-applications, handle the migration of existing on-premise applications to the cloud, and debug cloud stacks.\n",
    "- required skills: ICT system integration, ICT system programming, SaaS (service-oriented modelling), cloud monitoring and reporting, cloud security and compliance, cloud technologies, computer programming, cyber security database, development tools, implement cloud security and compliance, operating systems, systems development life-cycle, systems theory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting skills from CVs\n",
    "\n",
    "This notebook shows how to create a spacy NLP model based on the ESCO skill descriptions and labels, for identifying skills in CVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ.update(\n",
    "    dict(\n",
    "        # PYTORCH_ROCM_ARCH=\"gfx90c\",\n",
    "        LD_LIBRARY_PATH=\"/opt/rocm/lib:/opt/rocm/libexec\",\n",
    "        # HSA_OVERRIDE_GFX_VERSION=\"90c\",\n",
    "        ROCM_HOME=\"/opt/rocm\",\n",
    "        ROCM_PATH=\"/opt/rocm\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Without GPU this is quite slow. Further info on using a GCP server on GPU can be found in GCP.md\n",
    "# spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ESCO data from SPARQL and create a dataframe with some useful columns.\n",
    "# Alternatively, use the pre-processed esco.json file.\n",
    "df = pd.read_json(\"../esco/esco.json.gz\", orient=\"records\")\n",
    "df.index = df.uri\n",
    "skills = df.groupby(df.uri).agg(\n",
    "    {\n",
    "        \"altLabel\": lambda x: list(set(x)),\n",
    "        \"label\": lambda x: x.iloc[0],\n",
    "        \"description\": lambda x: x.iloc[0],\n",
    "        \"skillType\": lambda x: x.iloc[0],\n",
    "    }\n",
    ")\n",
    "# Add a lowercase text field for semantic search.\n",
    "skills[\"text\"] = skills.apply(\n",
    "    lambda x: \"; \".join([x.label] + x.altLabel + [x.description]).lower(), axis=1\n",
    ")\n",
    "# .. and a set of all the labels for each skill.\n",
    "skills[\"allLabel\"] = skills.apply(\n",
    "    lambda x: {t.lower() for t in x.altLabel} | {x.label.lower()}, axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.option_context(\"max_colwidth\", None):\n",
    "  display(skills.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Smoke test some skills\n",
    "list(skills[skills.label.str.contains(\"MySQL\")].altLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esco import to_curie, from_curie\n",
    "\n",
    "def make_pattern(id_: str, kn: dict):\n",
    "    \"\"\"Given an ESCO skill entry in the dataframe, create a pattern for the matcher.\n",
    "\n",
    "    The entry has the following fields:\n",
    "    - label: the preferred label\n",
    "    - altLabel: a list of alternative labels\n",
    "    - the skillType: e.g. knowledge, skill, ability\n",
    "\n",
    "    The logic uses some euristic to decide whether to use the preferred label or the alternative labels.\n",
    "    \"\"\"\n",
    "    label = kn[\"label\"]\n",
    "    pattern = [{\"LOWER\": label.lower()}] if len(label) > 3 else [{\"TEXT\": label}]\n",
    "    patterns = [pattern]\n",
    "    altLabel = [kn[\"altLabel\"]] if isinstance(kn[\"altLabel\"], str) else kn[\"altLabel\"]\n",
    "    for alt in altLabel:\n",
    "        # If the label is a 3-letter word, use an exact match.\n",
    "        if len(alt) <= 3:\n",
    "            candidate = [{\"TEXT\": alt}]\n",
    "        \n",
    "        # If there are up to 3 words, use a lowercase match.\n",
    "        elif 1 < len(alt.split()) <= 3:\n",
    "            candidate = [{\"LOWER\": x} for x in alt.lower().split()]\n",
    "\n",
    "        # Otherwise use a lowercase match with the whole string.\n",
    "        # Maybe:\n",
    "        # - use a lemma match\n",
    "        # - skip this case, and use a full-text/semantic search\n",
    "        else:\n",
    "            candidate = [{\"LOWER\": alt.lower()}]\n",
    "        if candidate not in patterns:\n",
    "            patterns.append(candidate)\n",
    "\n",
    "    # The following identifier is not used anymore, but it's kept here for reference.\n",
    "    pattern_identifier = (\n",
    "        f\"{kn['skillType'][:2]}_{label.replace(' ', '_')}\".upper().translate(\n",
    "            str.maketrans(\"\", \"\", \"()\")\n",
    "        )\n",
    "    )\n",
    "    return to_curie(id_), patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(id_, kni) for id_, kni in skills.to_dict(orient=\"index\").items() if \"software archit\" in str(kni) ][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the patterns for the matcher\n",
    "m = dict(\n",
    "    make_pattern(id_, kni)\n",
    "    for id_, kni in skills.to_dict(orient=\"index\").items()\n",
    "    # if \"architecture\" in str(kni[\"altLabel\"])\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy matcher with a blank model to validate the patterns.\n",
    "# If this doesn't work, spacy will raise an error.\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp_test = spacy.blank(\"en\")\n",
    "m1 = Matcher(nlp_test.vocab, validate=True)\n",
    "for pid, patterns in m.items():\n",
    "    m1.add(pid, patterns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 3 patterns\n",
    "list(m.items())[:3]\n",
    "json.dump(m, open(\"../generated/esco_matchers.json\", \"w\"))\n",
    "esco_p = [{\"label\":\"ESCO\", \"pattern\": pattern, \"id\": k } for k, p in m.items()  for pattern in p ]\n",
    "\n",
    "# Save the patterns to a json\n",
    "import json\n",
    "with open(\"../generated/esco_patterns.json\", \"w\") as f:\n",
    "    json.dump(esco_p, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an ESCO spacy entity recognizer model\n",
    "\n",
    "This entity recognizer reuses the en_core_web_trf model, that is quite good at identifying PRODUCTS.\n",
    "We will add a new entity label, ESCO, that uses the altLabel patterns to identify further entities.\n",
    "\n",
    "The ESCO entity label is added to the pipeline after the NER component, so that the NER component can identify the entities that are already in the en_core_web_trf model, and the ESCO component can add the ESCO entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is quite good at recognizing ICT entities like products.\n",
    "import spacy\n",
    "from spacy import displacy  # Load a viewer.\n",
    "nlp_e = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a custom tokenizer that preserves dashed words, so that the ESCO entity recognizer can identify them.\n",
    "\n",
    ":warning: **Note**: this is disabled for now, since it should be tested further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... eventually add a custom tokenizer ...\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "if False:\n",
    "    nlp_e.tokenizer = custom_tokenizer(nlp_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. then the entity ruler ..\n",
    "ruler = nlp_e.add_pipe(\"entity_ruler\", after=\"ner\")\n",
    "ruler.add_patterns(esco_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp_e.to_disk(\"../generated/en_core_web_trf_esco_ner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model\n",
    "\n",
    "Let's try the model on some sample texts:\n",
    "\n",
    "- a minimal text with a single skill\n",
    "- a complete CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from hashlib import sha256\n",
    "text = \"\"\"\n",
    "I design rest API using the openapi specifications.\n",
    "\n",
    "I daily use linux, courier-imap, openapi, openshift-on-openstack and mysql\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_stats(doc):\n",
    "    doc_id = sha256(doc.text.encode(\"utf-8\")).hexdigest()\n",
    "    all_entities = [(ent.text, ent.label_, ent.ent_id_) for ent in doc.ents]\n",
    "    ent_count = len(all_entities)\n",
    "    ent_unique = set(all_entities)\n",
    "    ent_unique_count = len(ent_unique)\n",
    "    ent_text_unique = set(text for text, _, _ in ent_unique)\n",
    "    ent_text_unique_count = len(ent_text_unique)\n",
    "    ent_skills = set((t, l, i) for t, l, i in ent_unique if l in (\"ESCO\", \"PRODUCT\"))\n",
    "    ent_skills_text_unique = len(set(text for text, _, _ in ent_skills))\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"ent_count\": ent_count,\n",
    "    #    \"ent_unique\": list(ent_unique),\n",
    "        \"ent_unique_count\": ent_unique_count,\n",
    "    #    \"ent_text_unique\": list(ent_text_unique),\n",
    "        \"ent_text_unique_count\": ent_text_unique_count,\n",
    "        \"ent_skills\": list(ent_skills),\n",
    "        \"ent_skills_text_unique\": ent_skills_text_unique,\n",
    "    }\n",
    "\n",
    "doc = nlp_e(text.replace(\"\\n\", \" \"))\n",
    "# Show some stats\n",
    "get_stats(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result.\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc.ents:\n",
    "    print(t.text, t.label_, t.ent_id_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "The recognizer is tested processing a set of CVs and returning the entities found in each CV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"data/\")\n",
    "cvs = list(DATADIR.glob(\"*-en.txt\"))\n",
    "len(cvs), DATADIR.absolute().as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the test.\n",
    "def model_factory(model_name, patterns=None, config=None):\n",
    "    model = spacy.load(model_name)\n",
    "    if patterns:\n",
    "        config = config or {}\n",
    "        ruler = model.add_pipe(\"entity_ruler\", **config)\n",
    "        ruler.add_patterns(esco_p)\n",
    "    return model\n",
    "\n",
    "\n",
    "testcases = {\n",
    "    \"base\": model_factory(\"en_core_web_md\"),\n",
    "    \"trf\": model_factory(\"en_core_web_trf\"),\n",
    "    \"trf_pre\": model_factory(\"en_core_web_trf\", esco_p, config={\"before\": \"ner\"}),\n",
    "    \"trf_post\": model_factory(\"en_core_web_trf\", esco_p, config={\"after\": \"ner\"}),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testcases and save each result asap, since the test can take a long time.\n",
    "results_out = Path(\"results.out\")\n",
    "with results_out.open(\"wb\") as fh:\n",
    "    fh.write(b\"[\")\n",
    "    for model_name, nlp_model in testcases.items():\n",
    "        for doc, cv in nlp_model.pipe([(cv.read_text(), cv.stem) for cv in cvs], as_tuples=True):\n",
    "            stats = get_stats(doc)\n",
    "            stats[\"model\"] = model_name\n",
    "            fh.write(json.dumps(stats).encode())\n",
    "            fh.write(b\",\\n\")\n",
    "    fh.write(b\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Path(\"results.out\").read_text()\n",
    "data=yaml.safe_load(results)\n",
    "df = pd.DataFrame(data)\n",
    "df[\"model\"] = df.index // 9\n",
    "# aggregate the dataframe above by doc_id, using the couple (model, ent_.._count) as columns\n",
    "# and the doc_id as the index\n",
    "results = df.groupby([\"doc_id\", \"model\"]).agg(list).unstack()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment the saved model\n",
    "\n",
    "Now that we have a saved model, we can experiment with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "import re\n",
    "from spacy import displacy\n",
    "\n",
    "DATADIR = Path(sys.path[0]).parent / \"tests\" / \"data\"\n",
    "\n",
    "nlp_esco = spacy.load(\"../generated/en_core_web_trf_esco_ner/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw = (DATADIR / \"rpolli.txt\").read_text()\n",
    "text = re.sub('\\n+','\\n',text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_esco(text.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start analyzing the parts of speech (POS)\n",
    "from collections import Counter\n",
    "most_common = lambda pos: Counter([ t.lemma_ for t in doc if t.pos_ == pos and len(t.text)> 2]).most_common(6) \n",
    "print(\"Most common verbs\", most_common(\"VERB\"))\n",
    "print(\"Most common nouns\", most_common(\"NOUN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now a nice display\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(doc):\n",
    "    doc_id = sha256(doc.text.encode(\"utf-8\")).hexdigest()\n",
    "    all_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    ent_count = len(all_entities)\n",
    "    ent_unique = set(all_entities)\n",
    "    ent_unique_count = len(ent_unique)\n",
    "    ent_text_unique = set(text for text, _ in ent_unique)\n",
    "    ent_text_unique_count = len(ent_text_unique)\n",
    "    ent_skills = set((t, l) for t, l in ent_unique if l in (\"ESCO\", \"PRODUCT\"))\n",
    "    ent_skills_text_unique = len(set(text for text, _ in ent_skills))\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"ent_count\": ent_count,\n",
    "        \"ent_unique\": list(ent_unique),\n",
    "        \"ent_unique_count\": ent_unique_count,\n",
    "        \"ent_text_unique\": list(ent_text_unique),\n",
    "        \"ent_text_unique_count\": ent_text_unique_count,\n",
    "        \"ent_skills\": list(ent_skills),\n",
    "        \"ent_skills_text_unique\": ent_skills_text_unique,\n",
    "    }\n",
    "\n",
    "get_stats(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some takings\n",
    "\n",
    "- The model is not perfect, but it is a good starting point for further improvements.\n",
    "- Even a simple lexical analysis can provide useful insights.\n",
    "- It is helpful to identify legacy skills that are no more relevant in the current market.\n",
    "- The model is good at identifying knowledges (e.g., programming languages, tools).\n",
    "- The model is not good at identifying skills made up of multiple words (e.g., \"can do X\"). The `esco` package addresses it using vector search.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "- Analyze the ESCO descriptions and labels and do some preprocessing to improve the patterns or the vector search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
