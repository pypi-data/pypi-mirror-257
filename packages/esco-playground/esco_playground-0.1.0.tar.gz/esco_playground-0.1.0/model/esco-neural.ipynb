{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify ESCO skills in CVs using neural search\n",
    "\n",
    "This notebook uses neural search to identify ESCO skills where `esco:skillType=skill`.\n",
    "Differently from `esco:skillType=knowledge`, these skills are expressed in natural language using multiple words and do not usually contain acronyms.\n",
    "\n",
    "Acronyms can be a problem for some neural search models, as they can be confused with other words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esco\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "\n",
    "# Load preliminary data.\n",
    "db = esco.LocalDB()\n",
    "skills : pd.DataFrame = db.skills\n",
    "S = db.skills[db.skills.skillType.str.endswith(\"skill\")]\n",
    "K = db.skills[db.skills.skillType.str.endswith(\"knowledge\")]\n",
    "sdb = esco.LocalDB()\n",
    "sdb.skills = S\n",
    "print(\"Using \", len(S), \" skills and \", len(K), \" knowledges\")\n",
    "assert len(sdb.search_products([\"Haskell\"])) == 0  # Ensure no knowledges: these should be matched using Spacy NER.\n",
    "\n",
    "cv_txt = Path(\"rpolli.txt\")\n",
    "cv_doc = nltk.sent_tokenize(cv_txt.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create various Vector Indexes for the ESCO skills\n",
    "\n",
    "Create embeddings with different models and store them into qdrant for local searches.\n",
    "By default Qdrant uses cosine distance.\n",
    "\n",
    "langchain simplifies this process with data connector classes for [different vector stores](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
    "e.g.  vector_db = VectorStore.from_documents(..)\n",
    "\n",
    "This VectorStore allows different search operations:\n",
    "\n",
    "- similarity_search(query, k, filter):\n",
    "- similarity_search_with_score(query, k=4, filter, score_threshold=None)\n",
    "- max_marginal_relevance_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "\n",
    "from langchain_community.vectorstores.qdrant import Qdrant\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "dbs = {}\n",
    "models = (\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"paraphrase-MiniLM-L3-v2\",\n",
    "    \"paraphrase-albert-small-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## FAISS (faster but less accurate)\n",
    "\n",
    "Use langchain and FAISS to create different vector indexes for the ESCO skills.\n",
    "[FAISS](https://faiss.ai) is a library for efficient similarity search and clustering of dense vectors based on Euclidean ($L^2$) distance.\n",
    "\n",
    "\n",
    "On colab with T4 GPU this takes:\n",
    "\n",
    "- Time for all-MiniLM-L12-v2 -->  1.9036436080932617\n",
    "- Time for paraphrase-albert-small-v2 -->  2.5741355419158936\n",
    "\n",
    "Coupling the given embedding functions with FAISS, the `paraphrase-albert-small-v2` model seems to be the best performing one.\n",
    "Note that, since the IT skill dataset is small (~600 skills), we could use less search libraries less efficient than FAISS to get better results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":warning: FAISS stopped working after switching to langchain_community. Since we don't use it, we don't fix it.\n",
    "\n",
    "for m in models:\n",
    "    start = time.time()\n",
    "    print(\"Load embedding function\", m)\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=m)\n",
    "    print(\"Generate embeddings\")\n",
    "    dbs[f\"{m}-FAISS\"] = FAISS.from_texts(\n",
    "        list(S.text.values),\n",
    "        embedding_function,\n",
    "        skills[[\"label\"]].to_dict(orient=\"records\"),\n",
    "    )\n",
    "    print(f\"Time for {m} --> \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv_path = Path(\"rpolli.txt\")\n",
    "cv_text = cv_path.read_text()\n",
    "print(\"CV: \", cv_path.stem, sep=\"\\n\")\n",
    "for model, db in dbs.items():\n",
    "    my_skills = [\n",
    "        db.search(str(part), search_type=\"mmr\") for part in nltk.sent_tokenize(cv_text)\n",
    "    ]\n",
    "    my_skills_c = Counter(x.metadata[\"label\"] for x in chain(*my_skills))\n",
    "    print(model, *my_skills_c.most_common(10), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant\n",
    "\n",
    "Create embeddings with different models and store them into qdrant for local searches.\n",
    "By default Qdrant uses cosine distance.\n",
    "\n",
    "langchain simplifies this process with data connector classes for [different vector stores](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
    "e.g.  vector_db = VectorStore.from_documents(..)\n",
    "\n",
    "This VectorStore allows different search operations:\n",
    "\n",
    "- similarity_search(query, k, filter):\n",
    "- similarity_search_with_score(query, k=4, filter, score_threshold=None)\n",
    "- max_marginal_relevance_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [\n",
    "    Document(page_content=t, metadata={\"label\": l, \"uri\": i})\n",
    "    for t, l, i in zip(S.text.values, S.label.values, S.index.values)\n",
    "]\n",
    "for m in models:\n",
    "    start = time.time()\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=m)\n",
    "    db = dbs[f\"qdrant-{m}\"] = Qdrant.from_documents(\n",
    "        documents,\n",
    "        embedding_function,\n",
    "        path=f\"qdrant-{m}\",\n",
    "        collection_name=\"esco\",\n",
    "        force_recreate=True,\n",
    "    )\n",
    "    print(f\"Time for {m} --> \", time.time() - start)\n",
    "    db.client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the ESCO embeddings\n",
    "\n",
    "We load the ESCO embeddings from Qdrant, then confront the results of matching CVs sentences against the ESCO embeddings.\n",
    "\n",
    "We test different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "assert S  # Did you run the previous cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models[2]\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=m)\n",
    "documents =  [Document(page_content=t, metadata={\"label\":i}) for t, i in zip(skills.text.values, skills.label.values)]\n",
    "\n",
    "# Don't modify the original data loaded from disk.\n",
    "Qdrant.add_texts = lambda *x: None\n",
    "qdb = Qdrant.from_documents(\n",
    "    documents[:1],\n",
    "    embedding_function,\n",
    "    path=f\"qdrant-{m}\",\n",
    "    collection_name=\"esco\",\n",
    "    force_recreate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For each CV sentence, we extract the top k ESCO skills using the cosine similarity between the CV sentence and the ESCO embeddings generated via an ST embedding model.\n",
    "\n",
    " The `distilbert` model returns a lot of false positives, but the similarity scores are higher.\n",
    "\n",
    " The `all-MiniLM-L6-v2` and `paraphrase-albert-small-v2` models perform better,\n",
    " but their similarity scores are lower 0.25-0.60 for good matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "results = {}\n",
    "for k_, db in dbs.items():\n",
    "    model_name = k_.replace(\"qdrant-\", \"\")\n",
    "    neural_cv = []\n",
    "    for sentence in cv_doc:\n",
    "        txt = str(sentence).strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        if len(txt.split()) < 5:\n",
    "            continue\n",
    "\n",
    "        neural_cv.append({\n",
    "            \"text\": txt,\n",
    "            \"skills\": [{\"label\": x[0].metadata[\"label\"], \"score\": x[1], \"uri\": x[0].metadata[\"uri\"]} for x in db.similarity_search_with_score(txt, k=10, score_threshold=0.25)]\n",
    "        })\n",
    "\n",
    "    results[k_] = list(chain(*(x[\"skills\"] for x in neural_cv)))\n",
    "    Path(f\"esco-neural-{k_}.yaml\").write_text(\n",
    "        yaml.dump(neural_cv)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for k, items in results.items():\n",
    "    results_dict[k_] = x = {}\n",
    "    for d, score in items:\n",
    "        id_ = d.metadata[\"uri\"]\n",
    "        if id_ not in x:\n",
    "            x[id_] = {\"label\": d.metadata[\"label\"], \"score\": score, \"count\": 1}\n",
    "        else:\n",
    "            x[id_][\"score\"] = max(score, x[id_][\"score\"])\n",
    "            x[id_][\"count\"] += 1\n",
    "    print(\"Neural skills found: \",k, len(x), max(x.values(), key=lambda x: x[\"score\"]), min(x.values(), key=lambda x: x[\"score\"]), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[db.client.close() for db in dbs.values()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
