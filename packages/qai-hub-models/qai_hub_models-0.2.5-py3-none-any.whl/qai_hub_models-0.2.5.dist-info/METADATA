Metadata-Version: 2.1
Name: qai-hub-models
Version: 0.2.5
Summary: Models optimized for export to run on device.
Home-page: https://github.com/quic/ai-hub-models
Author: Qualcomm® Technologies, Inc.
License: MIT
Platform: UNKNOWN
Requires-Python: >=3.8, <3.11
Description-Content-Type: text/markdown
Requires-Dist: Pillow ==10.0.1
Requires-Dist: gdown ==4.7.1
Requires-Dist: gitpython
Requires-Dist: huggingface-hub
Requires-Dist: ipython
Requires-Dist: numpy ==1.23.1
Requires-Dist: opencv-python ==4.8.1.78
Requires-Dist: prettytable
Requires-Dist: pytest ==7.4.2
Requires-Dist: pyyaml
Requires-Dist: qai-hub
Requires-Dist: requests
Requires-Dist: requests-toolbelt
Requires-Dist: schema
Requires-Dist: torch ==1.13.1
Requires-Dist: torchvision <=0.14.1
Requires-Dist: urllib3 <2
Provides-Extra: controlnet-quantized
Requires-Dist: transformers ==4.31.0 ; extra == 'controlnet-quantized'
Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'controlnet-quantized'
Requires-Dist: opencv-python ==4.8.1.78 ; extra == 'controlnet-quantized'
Provides-Extra: controlnet_quantized
Requires-Dist: transformers ==4.31.0 ; extra == 'controlnet_quantized'
Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'controlnet_quantized'
Requires-Dist: opencv-python ==4.8.1.78 ; extra == 'controlnet_quantized'
Provides-Extra: detr-resnet101
Requires-Dist: transformers ==4.31.0 ; extra == 'detr-resnet101'
Requires-Dist: timm ==0.9.7 ; extra == 'detr-resnet101'
Provides-Extra: detr-resnet101-dc5
Requires-Dist: transformers ==4.31.0 ; extra == 'detr-resnet101-dc5'
Requires-Dist: timm ==0.9.7 ; extra == 'detr-resnet101-dc5'
Provides-Extra: detr-resnet50
Requires-Dist: transformers ==4.31.0 ; extra == 'detr-resnet50'
Requires-Dist: timm ==0.9.7 ; extra == 'detr-resnet50'
Provides-Extra: detr-resnet50-dc5
Requires-Dist: transformers ==4.31.0 ; extra == 'detr-resnet50-dc5'
Requires-Dist: timm ==0.9.7 ; extra == 'detr-resnet50-dc5'
Provides-Extra: detr_resnet101
Requires-Dist: transformers ==4.31.0 ; extra == 'detr_resnet101'
Requires-Dist: timm ==0.9.7 ; extra == 'detr_resnet101'
Provides-Extra: detr_resnet101_dc5
Requires-Dist: transformers ==4.31.0 ; extra == 'detr_resnet101_dc5'
Requires-Dist: timm ==0.9.7 ; extra == 'detr_resnet101_dc5'
Provides-Extra: detr_resnet50
Requires-Dist: transformers ==4.31.0 ; extra == 'detr_resnet50'
Requires-Dist: timm ==0.9.7 ; extra == 'detr_resnet50'
Provides-Extra: detr_resnet50_dc5
Requires-Dist: transformers ==4.31.0 ; extra == 'detr_resnet50_dc5'
Requires-Dist: timm ==0.9.7 ; extra == 'detr_resnet50_dc5'
Provides-Extra: dev
Requires-Dist: boto3 ; extra == 'dev'
Requires-Dist: botocore ; extra == 'dev'
Requires-Dist: coverage ==6.5.0 ; extra == 'dev'
Requires-Dist: huggingface-hub ==0.20.3 ; extra == 'dev'
Requires-Dist: mypy ==0.991 ; extra == 'dev'
Requires-Dist: protobuf ==3.20.3 ; extra == 'dev'
Requires-Dist: pytest-cov ==4.1.0 ; extra == 'dev'
Requires-Dist: pytest-xdist ==3.3.1 ; extra == 'dev'
Requires-Dist: pyyaml ==6.0.1 ; extra == 'dev'
Requires-Dist: ruamel-yaml ; extra == 'dev'
Requires-Dist: schema ==0.7.5 ; extra == 'dev'
Requires-Dist: scikit-image >=0.21.0 ; extra == 'dev'
Requires-Dist: types-PyYAML ; extra == 'dev'
Requires-Dist: types-pillow ; extra == 'dev'
Requires-Dist: types-requests ; extra == 'dev'
Requires-Dist: tensorflow-cpu ==2.13.0 ; (sys_platform != "darwin") and extra == 'dev'
Requires-Dist: tensorflow-macos ==2.13.0 ; (sys_platform == "darwin") and extra == 'dev'
Provides-Extra: facebook-denoiser
Requires-Dist: denoiser ; extra == 'facebook-denoiser'
Requires-Dist: torchaudio ; extra == 'facebook-denoiser'
Requires-Dist: PySoundFile ; (sys_platform == "win32") and extra == 'facebook-denoiser'
Provides-Extra: facebook_denoiser
Requires-Dist: denoiser ; extra == 'facebook_denoiser'
Requires-Dist: torchaudio ; extra == 'facebook_denoiser'
Requires-Dist: PySoundFile ; (sys_platform == "win32") and extra == 'facebook_denoiser'
Provides-Extra: fastsam-s
Requires-Dist: ultralytics ==8.0.193 ; extra == 'fastsam-s'
Requires-Dist: torchvision ; extra == 'fastsam-s'
Provides-Extra: fastsam-x
Requires-Dist: ultralytics ==8.0.193 ; extra == 'fastsam-x'
Requires-Dist: torchvision ; extra == 'fastsam-x'
Provides-Extra: fastsam_s
Requires-Dist: ultralytics ==8.0.193 ; extra == 'fastsam_s'
Requires-Dist: torchvision ; extra == 'fastsam_s'
Provides-Extra: fastsam_x
Requires-Dist: ultralytics ==8.0.193 ; extra == 'fastsam_x'
Requires-Dist: torchvision ; extra == 'fastsam_x'
Provides-Extra: ffnet-122ns-lowres
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet-122ns-lowres'
Provides-Extra: ffnet-40s
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet-40s'
Provides-Extra: ffnet-54s
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet-54s'
Provides-Extra: ffnet-78s
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet-78s'
Provides-Extra: ffnet-78s-lowres
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet-78s-lowres'
Provides-Extra: ffnet_122ns_lowres
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet_122ns_lowres'
Provides-Extra: ffnet_40s
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet_40s'
Provides-Extra: ffnet_54s
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet_54s'
Provides-Extra: ffnet_78s
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet_78s'
Provides-Extra: ffnet_78s_lowres
Requires-Dist: scikit-image >=0.21.0 ; extra == 'ffnet_78s_lowres'
Provides-Extra: fomm
Requires-Dist: scipy >=1.10.1 ; extra == 'fomm'
Requires-Dist: imageio[ffmpeg] >=2.31.5 ; extra == 'fomm'
Requires-Dist: ffmpeg >=0.2.0 ; extra == 'fomm'
Requires-Dist: matplotlib >=3.7.3 ; extra == 'fomm'
Requires-Dist: pandas >=2.0.3 ; extra == 'fomm'
Requires-Dist: scikit-image >=0.21.0 ; extra == 'fomm'
Requires-Dist: scikit-learn >=1.1.2 ; extra == 'fomm'
Provides-Extra: hrnet-pose
Requires-Dist: yacs ==0.1.8 ; extra == 'hrnet-pose'
Requires-Dist: mmpose <=1.2.0 ; extra == 'hrnet-pose'
Requires-Dist: mmcv ==2.1.0 ; extra == 'hrnet-pose'
Requires-Dist: mmdet <=3.2.0 ; extra == 'hrnet-pose'
Provides-Extra: hrnet-pose-quantized
Requires-Dist: yacs ==0.1.8 ; extra == 'hrnet-pose-quantized'
Requires-Dist: mmpose <=1.2.0 ; extra == 'hrnet-pose-quantized'
Requires-Dist: mmcv ==2.1.0 ; extra == 'hrnet-pose-quantized'
Requires-Dist: mmdet <=3.2.0 ; extra == 'hrnet-pose-quantized'
Provides-Extra: hrnet_pose
Requires-Dist: yacs ==0.1.8 ; extra == 'hrnet_pose'
Requires-Dist: mmpose <=1.2.0 ; extra == 'hrnet_pose'
Requires-Dist: mmcv ==2.1.0 ; extra == 'hrnet_pose'
Requires-Dist: mmdet <=3.2.0 ; extra == 'hrnet_pose'
Provides-Extra: hrnet_pose_quantized
Requires-Dist: yacs ==0.1.8 ; extra == 'hrnet_pose_quantized'
Requires-Dist: mmpose <=1.2.0 ; extra == 'hrnet_pose_quantized'
Requires-Dist: mmcv ==2.1.0 ; extra == 'hrnet_pose_quantized'
Requires-Dist: mmdet <=3.2.0 ; extra == 'hrnet_pose_quantized'
Provides-Extra: huggingface-wavlm-base-plus
Requires-Dist: transformers >=4.31.0 ; extra == 'huggingface-wavlm-base-plus'
Requires-Dist: soundfile >=0.12.1 ; extra == 'huggingface-wavlm-base-plus'
Requires-Dist: librosa >=0.10.1 ; extra == 'huggingface-wavlm-base-plus'
Requires-Dist: datasets >=2.14.5 ; extra == 'huggingface-wavlm-base-plus'
Provides-Extra: huggingface_wavlm_base_plus
Requires-Dist: transformers >=4.31.0 ; extra == 'huggingface_wavlm_base_plus'
Requires-Dist: soundfile >=0.12.1 ; extra == 'huggingface_wavlm_base_plus'
Requires-Dist: librosa >=0.10.1 ; extra == 'huggingface_wavlm_base_plus'
Requires-Dist: datasets >=2.14.5 ; extra == 'huggingface_wavlm_base_plus'
Provides-Extra: lama-dilated
Requires-Dist: matplotlib ; extra == 'lama-dilated'
Requires-Dist: pandas ; extra == 'lama-dilated'
Requires-Dist: albumentations ==0.5.2 ; extra == 'lama-dilated'
Requires-Dist: pytorch-lightning ==1.6.0 ; extra == 'lama-dilated'
Requires-Dist: webdataset ; extra == 'lama-dilated'
Requires-Dist: easydict ==1.10 ; extra == 'lama-dilated'
Requires-Dist: kornia ==0.5.0 ; extra == 'lama-dilated'
Requires-Dist: hydra-core ==1.3.0 ; extra == 'lama-dilated'
Requires-Dist: omegaconf ==2.3.0 ; extra == 'lama-dilated'
Requires-Dist: scikit-learn ==1.3.0 ; extra == 'lama-dilated'
Provides-Extra: lama_dilated
Requires-Dist: matplotlib ; extra == 'lama_dilated'
Requires-Dist: pandas ; extra == 'lama_dilated'
Requires-Dist: albumentations ==0.5.2 ; extra == 'lama_dilated'
Requires-Dist: pytorch-lightning ==1.6.0 ; extra == 'lama_dilated'
Requires-Dist: webdataset ; extra == 'lama_dilated'
Requires-Dist: easydict ==1.10 ; extra == 'lama_dilated'
Requires-Dist: kornia ==0.5.0 ; extra == 'lama_dilated'
Requires-Dist: hydra-core ==1.3.0 ; extra == 'lama_dilated'
Requires-Dist: omegaconf ==2.3.0 ; extra == 'lama_dilated'
Requires-Dist: scikit-learn ==1.3.0 ; extra == 'lama_dilated'
Provides-Extra: litehrnet
Requires-Dist: mmpose <=1.2.0 ; extra == 'litehrnet'
Requires-Dist: mmcv ==2.1.0 ; extra == 'litehrnet'
Requires-Dist: mmdet <=3.2.0 ; extra == 'litehrnet'
Provides-Extra: mediapipe-face
Requires-Dist: opencv-python ; extra == 'mediapipe-face'
Requires-Dist: requests ; extra == 'mediapipe-face'
Provides-Extra: mediapipe-hand
Requires-Dist: opencv-python ; extra == 'mediapipe-hand'
Requires-Dist: requests ; extra == 'mediapipe-hand'
Provides-Extra: mediapipe-pose
Requires-Dist: opencv-python ; extra == 'mediapipe-pose'
Requires-Dist: requests ; extra == 'mediapipe-pose'
Provides-Extra: mediapipe-selfie
Requires-Dist: tflite ==2.10.0 ; extra == 'mediapipe-selfie'
Provides-Extra: mediapipe_face
Requires-Dist: opencv-python ; extra == 'mediapipe_face'
Requires-Dist: requests ; extra == 'mediapipe_face'
Provides-Extra: mediapipe_hand
Requires-Dist: opencv-python ; extra == 'mediapipe_hand'
Requires-Dist: requests ; extra == 'mediapipe_hand'
Provides-Extra: mediapipe_pose
Requires-Dist: opencv-python ; extra == 'mediapipe_pose'
Requires-Dist: requests ; extra == 'mediapipe_pose'
Provides-Extra: mediapipe_selfie
Requires-Dist: tflite ==2.10.0 ; extra == 'mediapipe_selfie'
Provides-Extra: mobiledet
Requires-Dist: tucker-conv ==1.0.1 ; extra == 'mobiledet'
Requires-Dist: timm ==0.6.13 ; extra == 'mobiledet'
Provides-Extra: openai-clip
Requires-Dist: torchvision ; extra == 'openai-clip'
Requires-Dist: ftfy ==6.1.1 ; extra == 'openai-clip'
Requires-Dist: regex ==2023.10.3 ; extra == 'openai-clip'
Provides-Extra: openai_clip
Requires-Dist: torchvision ; extra == 'openai_clip'
Requires-Dist: ftfy ==6.1.1 ; extra == 'openai_clip'
Requires-Dist: regex ==2023.10.3 ; extra == 'openai_clip'
Provides-Extra: openpose
Requires-Dist: scipy ; extra == 'openpose'
Requires-Dist: matplotlib ; extra == 'openpose'
Provides-Extra: real-esrgan-general-x4v3
Requires-Dist: opencv-python ; extra == 'real-esrgan-general-x4v3'
Requires-Dist: PyYAML ; extra == 'real-esrgan-general-x4v3'
Requires-Dist: requests ; extra == 'real-esrgan-general-x4v3'
Requires-Dist: scipy ; extra == 'real-esrgan-general-x4v3'
Requires-Dist: seaborn ; extra == 'real-esrgan-general-x4v3'
Requires-Dist: basicsr ; extra == 'real-esrgan-general-x4v3'
Provides-Extra: real-esrgan-x4plus
Requires-Dist: opencv-python ; extra == 'real-esrgan-x4plus'
Requires-Dist: scipy ; extra == 'real-esrgan-x4plus'
Requires-Dist: seaborn ; extra == 'real-esrgan-x4plus'
Requires-Dist: basicsr ; extra == 'real-esrgan-x4plus'
Provides-Extra: real_esrgan_general_x4v3
Requires-Dist: opencv-python ; extra == 'real_esrgan_general_x4v3'
Requires-Dist: PyYAML ; extra == 'real_esrgan_general_x4v3'
Requires-Dist: requests ; extra == 'real_esrgan_general_x4v3'
Requires-Dist: scipy ; extra == 'real_esrgan_general_x4v3'
Requires-Dist: seaborn ; extra == 'real_esrgan_general_x4v3'
Requires-Dist: basicsr ; extra == 'real_esrgan_general_x4v3'
Provides-Extra: real_esrgan_x4plus
Requires-Dist: opencv-python ; extra == 'real_esrgan_x4plus'
Requires-Dist: scipy ; extra == 'real_esrgan_x4plus'
Requires-Dist: seaborn ; extra == 'real_esrgan_x4plus'
Requires-Dist: basicsr ; extra == 'real_esrgan_x4plus'
Provides-Extra: resnet-2plus1d
Requires-Dist: av ==10.0.0 ; extra == 'resnet-2plus1d'
Provides-Extra: resnet-3d
Requires-Dist: av ==10.0.0 ; extra == 'resnet-3d'
Provides-Extra: resnet-mixed
Requires-Dist: av ==10.0.0 ; extra == 'resnet-mixed'
Provides-Extra: resnet_2plus1d
Requires-Dist: av ==10.0.0 ; extra == 'resnet_2plus1d'
Provides-Extra: resnet_3d
Requires-Dist: av ==10.0.0 ; extra == 'resnet_3d'
Provides-Extra: resnet_mixed
Requires-Dist: av ==10.0.0 ; extra == 'resnet_mixed'
Provides-Extra: sam
Requires-Dist: matplotlib ; extra == 'sam'
Requires-Dist: opencv-python ; extra == 'sam'
Requires-Dist: pycocotools ; extra == 'sam'
Requires-Dist: requests ; extra == 'sam'
Provides-Extra: stable-diffusion
Requires-Dist: transformers ==4.31.0 ; extra == 'stable-diffusion'
Requires-Dist: coremltools ==6.2 ; extra == 'stable-diffusion'
Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable-diffusion'
Provides-Extra: stable-diffusion-quantized
Requires-Dist: transformers ==4.31.0 ; extra == 'stable-diffusion-quantized'
Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable-diffusion-quantized'
Provides-Extra: stable_diffusion
Requires-Dist: transformers ==4.31.0 ; extra == 'stable_diffusion'
Requires-Dist: coremltools ==6.2 ; extra == 'stable_diffusion'
Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable_diffusion'
Provides-Extra: stable_diffusion_quantized
Requires-Dist: transformers ==4.31.0 ; extra == 'stable_diffusion_quantized'
Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable_diffusion_quantized'
Provides-Extra: stylegan2
Requires-Dist: click >=8.0 ; extra == 'stylegan2'
Provides-Extra: trocr
Requires-Dist: transformers ==4.33.2 ; extra == 'trocr'
Requires-Dist: sentencepiece ; extra == 'trocr'
Provides-Extra: whisper-asr
Requires-Dist: openai-whisper ==20230314 ; extra == 'whisper-asr'
Requires-Dist: scipy ; extra == 'whisper-asr'
Provides-Extra: whisper_asr
Requires-Dist: openai-whisper ==20230314 ; extra == 'whisper_asr'
Requires-Dist: scipy ; extra == 'whisper_asr'
Provides-Extra: yolov7
Requires-Dist: matplotlib ; extra == 'yolov7'
Requires-Dist: opencv-python ; extra == 'yolov7'
Requires-Dist: PyYAML ; extra == 'yolov7'
Requires-Dist: requests ; extra == 'yolov7'
Requires-Dist: scipy ; extra == 'yolov7'
Requires-Dist: seaborn ; extra == 'yolov7'
Provides-Extra: yolov8-det
Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8-det'
Provides-Extra: yolov8-seg
Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8-seg'
Provides-Extra: yolov8_det
Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8_det'
Provides-Extra: yolov8_seg
Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8_seg'

[![Qualcomm® AI Hub Models](https://qaihub-public-assets.s3.us-west-2.amazonaws.com/qai-hub-models/quic-logo.jpg)](https://aihub.qualcomm.com)

# Qualcomm® AI Hub Models

The [Qualcomm® AI Hub Models](https://aihub.qualcomm.com/) are a collection of
state-of-the-art machine learning models optimized for performance (latency,
memory etc.) and ready to deploy on Qualcomm® devices.

* Explore models optimized for on-device deployment of vision, speech, text, and genenrative AI.
* View open-source recipes to quantize, optimize, and deploy these models on-device.
* Browse through [performance metrics](https://aihub.qualcomm.com/models) captured for these models on several devices.
* Access the models through [Hugging Face](https://huggingface.co/qualcomm).
* [Sign up](https://aihub.qualcomm.com/) to run these models on hosted Qualcomm® devices.

Supported runtimes
* [TensorFlow Lite](https://www.tensorflow.org/lite)
* [Qualcomm AI Engine Direct](https://www.qualcomm.com/developer/artificial-intelligence#overview)

Supported operating systems:
* Android 11+

Supported compute units
* CPU, GPU, NPU (includes [Hexagon DSP](https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor), [HTP](https://developer.qualcomm.com/hardware/qualcomm-innovators-development-kit/ai-resources-overview/ai-hardware-cores-accelerators))

Supported precision
* Floating Points: FP16
* Integer: INT8 (8-bit weight and activation on select models), INT4 (4-bit weight, 16-bit activation on select models)

Supported chipsets
* [Snapdragon 845](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-845-mobile-platform), [Snapdragon 855/855+](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-855-mobile-platform), [Snapdragon 865/865+](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-865-plus-5g-mobile-platform), [Snapdragon 888/888+](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-888-5g-mobile-platform)
* [Snapdragon 8 Gen 1](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-1-mobile-platform), [Snapdragon 8 Gen 2](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-2-mobile-platform), [Snapdragon 8 Gen 3](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform)

Select supported devices
* Samsung Galaxy S21 Series, Galaxy S22 Series, Galaxy S23 Series, Galaxy S24 Series
* Xiaomi 12, 13
* Google Pixel 3, 4, 5

and many more.

## Installation

We currently support **Python >=3.8 and <= 3.10.** We recommend using a Python
virtual environment
([miniconda](https://docs.anaconda.com/free/miniconda/miniconda-install/) or
[virtualenv](https://virtualenv.pypa.io/en/latest/)).

You can setup a virtualenv using:
```
python -m venv qai_hub_models_env && source qai_hub_models_env/bin/activate
```

Once the environment is setup, you can install the base package using:

```shell
pip install qai_hub_models
```

Some models (e.g. [YOLOv7](https://github.com/WongKinYiu/yolov7)) require
additional dependencies. You can install those dependencies automatically
using:

```shell
pip install "qai_hub_models[yolov7]"
```

## Getting Started

Each model comes with the following set of CLI demos:
* Locally runnable PyTorch based CLI demo to validate the model off device.
* On-device CLI demo that produces a model ready for on-device deployment and runs the model on a hosted Qualcomm® device (needs [sign up](https://aihub.qualcomm.com/)).

All the models produced by these demos are freely available on [Hugging
Face](https://huggingface.co/qualcomm) or through our
[website](https://aihub.qualcomm.com/models). See the individual model readme
files (e.g. [YOLOv7](qai_hub_models/models/yolov7/README.md)) for more
details.

### Local CLI Demo with PyTorch

[All models](#model-directory) contain CLI demos that run the model in
**PyTorch** locally with sample input. Demos are optimized for code clarity
rather than latency, and run exclusively in PyTorch. Optimal model latency can
be achieved with model export via [Qualcomm® AI
Hub](https://www.aihub.qualcomm.com).

```shell
python -m qai_hub_models.models.yolov7.demo
```

For additional details on how to use the demo CLI, use the `--help` option
```shell
python -m qai_hub_models.models.yolov7.demo --help
```

See the [model directory](#model-directory) below to explore all other models.

---

Note that most ML use cases require some pre and post-processing that are not
part of the model itself. A python reference implementation of this is provided
for each model in `app.py`. Apps load & pre-process model input, run model
inference, and post-process model output before returning it to you.

Here is an example of how the PyTorch CLI works for [YOLOv7](https://github.com/WongKinYiu/yolov7):

```python
from PIL import Image
from qai_hub_models.models.yolov7 import Model as YOLOv7Model
from qai_hub_models.models.yolov7 import App as YOLOv7App
from qai_hub_models.utils.asset_loaders import load_image
from qai_hub_models.models.yolov7.demo import IMAGE_ADDRESS

# Load pre-trained model
torch_model = YOLOv7Model.from_pretrained()

# Load a simple PyTorch based application
app = YOLOv7App(torch_model)
image = load_image(IMAGE_ADDRESS, "yolov7")

# Perform prediction on a sample image
pred_image = app.predict(image)[0]
Image.fromarray(pred_image).show()

```

### CLI demo to run on hosted Qualcomm® devices

[Some models](#model-directory) contain CLI demos that run the model on a hosted
Qualcomm® device using [Qualcomm® AI Hub](https://aihub.qualcomm.com).

To run the model on a hosted device, [sign up for access to Qualcomm® AI
Hub](https://aihub.qualcomm.com). Sign-in to Qualcomm® AI Hub with your
Qualcomm® ID. Once signed in navigate to Account -> Settings -> API Token.

With this API token, you can configure your client to run models on the cloud
hosted devices.

```shell
qai-hub configure --api_token API_TOKEN
```
Navigate to [docs](https://app.aihub.qualcomm.com/docs/) for more information.

The on-device CLI demo performs the following:
* Exports the model for on-device execution.
* Profiles the model on-device on a cloud hosted Qualcomm® device.
* Runs the model on-device on a cloud hosted Qualcomm® device and compares accuracy between a local CPU based PyTorch run and the on-device run.
* Downloads models (and other required assets) that can be deployed on-device in an Android application.

```shell
python -m qai_hub_models.models.yolov7.export
```

Many models may have initialization parameters that allow loading custom
weights and checkpoints. See `--help` for more details

```shell
python -m qai_hub_models.models.yolov7.export --help
```

#### How does this export script work?

As described above, the script above compiles, optimizes, and runs the model on
a cloud hosted Qualcomm® device. The demo uses [Qualcomm® AI Hub's Python
APIs](https://app.aihub.qualcomm.com/docs/).

<img src="https://app.aihub.qualcomm.com/docs/_images/How-It-Works.png" alt="Qualcomm® AI Hub explained" width="90%">

Here is a simplified example of code that can be used to run the entire model
on a cloud hosted device:

```python
from typing import Tuple
import torch
import qai_hub as hub
from qai_hub_models.models.yolov7 import Model as YOLOv7Model

# Load YOLOv7 in PyTorch
torch_model = YOLOv7Model.from_pretrained()
torch_model.eval()

# Trace the PyTorch model using one data point of provided sample inputs to
# torch tensor to trace the model.
example_input = [torch.tensor(data[0]) for name, data in torch_model.sample_inputs().items()]
pt_model = torch.jit.trace(torch_model, example_input)

# Select a device
device = hub.Device("Samsung Galaxy S23")

# Compile model on a specific device
compile_job = hub.submit_compile_job(
    model=pt_model,
    device=device,
    input_specs=torch_model.get_input_spec(),
)

# Get target model to run on a cloud hosted device
target_model = compile_job.get_target_model()

# Profile the previously compiled model
profile_job = hub.submit_profile_job(
    model=target_model,
    device=device,
)

# Perform on-device inference on the cloud hosted device
input_data = torch_model.sample_inputs()
inference_job = hub.submit_inference_job(
    model=target_model,
    device=device,
    inputs=input_data,
)

# Returns the output as dict{name: numpy}
on_device_output = inference_job.download_output_data()
```

---

### Working with source code

You can clone the repository using:

```shell
git clone {REPOSITORY_URL}
cd {REPOSITORY_ROOT_FILE_NAME}
pip install -e .
```

Install additional dependencies to prepare a model before using the following:
```shell
cd {REPOSITORY_ROOT_FILE_NAME}
pip install -e ".[yolov7]"
```

All models have accuracy and end-to-end tests when applicable. These tests as
designed to be run locally and verify that the PyTorch code produces correct
results.  To run the tests for a model:
```shell
python -m pytest --pyargs qai_hub_models.models.yolov7.test
```
---

For any issues, please contact us at qai-hub-support@qti.qualcomm.com.

---

## Model Directory


